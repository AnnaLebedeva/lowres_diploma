{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c25674c",
   "metadata": {},
   "source": [
    "# Building the vocabularies for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debabdf4",
   "metadata": {},
   "source": [
    "Here I present the pipeline for making vocabularies for the machine translation model from the data.  \n",
    "I have the following files for parent model:  \n",
    "- 1M.ru: file with 1 million Russian sentences  \n",
    "- 1M.chv: file with 1 million Chuvash sentences  \n",
    "\n",
    "This is the data I have for child model:  \n",
    "- khakas_ru.txt: file with 39523 Russian sentences  \n",
    "- khakas_kh.txt: file with 39523 Khakas sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7fd09",
   "metadata": {},
   "source": [
    "## The preparation of the text files for translation model must include the following steps:  \n",
    "### 1. checking for empty lines  \n",
    "\n",
    "I know for sure that Ru-Kh data doesn't include empty lines because I prepared the corpora. Now I will check Ru-Chv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca1f46e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "! grep -cvE '[^[:space:]]' 1M.ru\n",
    "! grep -cvE '[^[:space:]]' 1M.chv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373eea0f",
   "metadata": {},
   "source": [
    "### 2. checking for repeated lines\n",
    "\n",
    "I checked the Ru-Chv files for repeated lines, using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! awk 'seen[$0]++ == 1 { lines[$0]; next } $0 in lines { print NR, $0; exit }' 1M.chv\n",
    "# 241307 — Тӗлӗнмелле.\n",
    "! awk 'seen[$0]++ == 1 { lines[$0]; next } $0 in lines { print NR, $0; exit }' 1M.ru\n",
    "# 43899 — Конечно.\n",
    "! awk 'seen[$0]++ == 2 { lines[$0]; next } $0 in lines { print NR, $0; exit }' 1M.ru\n",
    "# 54716 Вот и все."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab217d61",
   "metadata": {},
   "source": [
    "There were repeated lines, but having looked at them closely, I figured that repeated lines in one language don't have duplicate in another language, but are rather translated with synonims, so it was not a mistake in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44083a",
   "metadata": {},
   "source": [
    "### 3. shuffling the lines\n",
    "It is important to shuffle the lines so that model trains better, and also data from different sources is represented in train, val and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34485a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! paste -d ':' 1M.ru 1M.chv | shuf | awk -v FS=\":\" '{ print $1 > \"1M_shuffed.ru\" ; print $2 > \"1M_shuffed.chv\" }'\n",
    "! paste -d ':' khakas_ru.txt khakas_kh.txt | shuf | awk -v FS=\":\" '{ print $1 > \"khakas_ru_shuffed.ru\" ; print $2 > \"khakas_kh_shuffed.chv\" }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958e226",
   "metadata": {},
   "source": [
    "### 4. Splitting the data into train, val and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf785dc9",
   "metadata": {},
   "source": [
    "We need to split these files into train, val and test.\n",
    "In the original article there were the fillowing sizes of the train-val-test files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb762f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22097622 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/train_file_parent.de\n",
      " 22097622 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/train_file_parent.cs\n",
      "    1997 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/data/de-cs_parallel/newstest2019-decs.cs\n",
      "    1997 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/data/de-cs_parallel/newstest2019-decs.de\n",
      "   60000 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/train.hsb-de.de\n",
      "   60000 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/train.hsb-de.hsb\n",
      "    2000 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/devel.hsb-de.de\n",
      "    2000 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/devel.hsb-de.hsb\n",
      "    2000 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/devel_test.hsb-de.de\n",
      "    2000 /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/devel_test.hsb-de.hsb\n"
     ]
    }
   ],
   "source": [
    "# train parent\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/train_file_parent.de\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/train_file_parent.cs\n",
    "# val parent\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/data/de-cs_parallel/newstest2019-decs.cs\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/data/de-cs_parallel/newstest2019-decs.de\n",
    "# train child\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/train.hsb-de.de\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/train.hsb-de.hsb\n",
    "# val child\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/devel.hsb-de.de\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/devel.hsb-de.hsb\n",
    "# test child\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/devel_test.hsb-de.de\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/WMT_repeat/parent_child/devel_test.hsb-de.hsb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db4555",
   "metadata": {},
   "source": [
    "The data I have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9241c3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1002013 /Users/macbook/Documents/NewFolder/Diplom/Rus_work/1M.chv\n",
      " 1002013 /Users/macbook/Documents/NewFolder/Diplom/Rus_work/1M.ru\n"
     ]
    }
   ],
   "source": [
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/Rus_work/1M.chv\n",
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/Rus_work/1M.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4d8750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39523 /Users/macbook/Documents/NewFolder/Diplom/Rus_work/khakas_kh.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l /Users/macbook/Documents/NewFolder/Diplom/Rus_work/khakas_kh.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9dfaa",
   "metadata": {},
   "source": [
    "In my case I guess it is ok to take 2000 as parent val, and 1000 lines for child val and child test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "head -n 1000013 1M_shuffed.ru > train_parent.ru\n",
    "tail -n +1000014 1M_shuffed.ru > val_parent.ru\n",
    "head -n 1000013 1M_shuffed.chv > train_parent.chv\n",
    "tail -n +1000014 1M_shuffed.chv > val_parent.chv\n",
    "\n",
    "head -n 38523 khakas_shuffed.ru > train_val_child.ru\n",
    "tail -n +38524 khakas_shuffed.ru > test_child.ru\n",
    "head -n 37523 train_val_child.ru > train_child.ru\n",
    "tail -n +37524 train_val_child.ru > val_child.ru\n",
    "\n",
    "head -n 38523 khakas_shuffed.kh > train_val_child.kh\n",
    "tail -n +38524 khakas_shuffed.kh > test_child.kh\n",
    "head -n 37523 train_val_child.kh > train_child.kh\n",
    "tail -n +37524 train_val_child.kh > val_child.kh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22827b0",
   "metadata": {},
   "source": [
    "## Building the vocabularies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea7ea1",
   "metadata": {},
   "source": [
    "After I prepared the necessary files, it is time to create vocabularies, that the model will use. According to the pipeline presented in the WMT article, I need to duplicate KH data 25 times, so that KH data is well represented in the vocab, and the KH tokens don't appear too rare to have a significant enough place in the vocab. I did it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639da8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! perl -0777pe '$_=$_ x 26' train_child.kh > 26train_child.kh\n",
    "! perl -0777pe '$_=$_ x 26' train_child.ru > 26train_child.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f6ff3",
   "metadata": {},
   "source": [
    "Then I concatenated all the files that I wanted to use for building the vocabularies the following way and got the combined files that I will later use in subword-nmt for building the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat train_parent.ru 26train_child.ru > train_vocab.ru\n",
    "! cat train_parent.chv 26train_child.kh > train_vocab.chvkh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5bc0e",
   "metadata": {},
   "source": [
    "All of the following code is done running RU_creating_vocabs_and_train_val_files.sh file and according to the subword-nmt tutorial (the installation of subword-nmt package is required).  \n",
    "First we perform learn-bpe operation on our training files. The parameters of this operation are taken from the WMT article. After that we perform apply-bpe operation on source and target files separately to get two separate vocabs:\n",
    "\n",
    "```python\n",
    "# learn BPE on concatenation of train files and extract vocabs \n",
    "cat train_vocab.ru train_vocab.chvkh | subword-nmt learn-bpe -s 20000 -o codes --min-frequency 2 --total-symbols\n",
    "\n",
    "subword-nmt apply-bpe -c codes < train_vocab.ru | subword-nmt get-vocab > vocab_file.ru\n",
    "subword-nmt apply-bpe -c codes < train_vocab.chvkh | subword-nmt get-vocab > vocab_file.chvkh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a9b37",
   "metadata": {},
   "source": [
    "We will explain the BPE operations in a separate notebook, here we will proceed with building the vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c2228",
   "metadata": {},
   "source": [
    "After getting the vocabs as a result of subword-nmt apply operation, we use the following script to make these vocabs compatible with sockeye model. The script is in the file subword-nmt_vocab2sockeye_vocab.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741ead2",
   "metadata": {},
   "source": [
    "```python\n",
    "# Diplom/WMT_repeat/parent_child/subword-nmt_vocab2sockeye_vocab.py\n",
    "#!/usr/bin/env  python3\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def getArgs():\n",
    "    usage=\"subword-nmt_vocac2sockeye_vocab.py  subword_nmt_vocab [sockeye_vocab_json]\"\n",
    "    help = ''\n",
    "    parser = ArgumentParser(usage=usage, description=help, add_help=True)\n",
    "    parser.add_argument(\"vocab_filename\", type=str, help=\"Subword-nmt's vocabulary filename\")\n",
    "    parser.add_argument(\"vocab_json\",\n",
    "         nargs   = '?',\n",
    "         type    = lambda f: open(f, mode='w', encoding='UTF-8'),\n",
    "         default = sys.stdout,\n",
    "         help    = \"output file [sys.stdout]\")\n",
    "    parser.add_argument(\"--add_bt_tag\",\n",
    "         dest    = \"add_bt_tag\",\n",
    "         action  = 'store_true',\n",
    "         default = False,\n",
    "         help    = \"Also include BT tag in vocab [%(default)s]\")\n",
    "    parser.add_argument(\"--add_generic_tags\",\n",
    "         dest    = \"add_generic_tags\",\n",
    "         action  = 'store_true',\n",
    "         default = False,\n",
    "         help    = \"Also include BT tag in vocab [%(default)s]\")\n",
    "    parser.add_argument(\"--add_glossaries\",\n",
    "         dest    = \"add_glossaries\",\n",
    "         nargs   = '?',\n",
    "         default = None,\n",
    "         help    = \"Also include the glossaries in vocab [%(default)s]\")\n",
    "\n",
    "\n",
    "    cmd_args = parser.parse_args()\n",
    "\n",
    "    return cmd_args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = getArgs()\n",
    "    vocab = dict({\n",
    "        \"<pad>\": 0,\n",
    "        \"<unk>\": 1,\n",
    "        \"<s>\": 2,\n",
    "        \"</s>\": 3,\n",
    "        })\n",
    "\n",
    "    if args.add_bt_tag:\n",
    "        vocab.update({'<BT>': len(vocab)})\n",
    "\n",
    "    if args.add_generic_tags:\n",
    "        vocab.update({ f'<TAG{i:02}>' : i+len(vocab)-1 for i in range(1, 26) })\n",
    "\n",
    "    with open(args.vocab_filename, mode='r', encoding='UTF-8') as f:\n",
    "        vocab.update({ k: v for v, k in enumerate(map(lambda l: l.strip().split()[0], f.readlines()), start=len(vocab.keys())) })\n",
    "\n",
    "    if args.add_glossaries is not None:\n",
    "        last_token_id = len(vocab.keys())\n",
    "        with open(args.add_glossaries, mode='rt') as f:\n",
    "            for token in map(str.strip, f.readlines()):\n",
    "                if token != '' and token not in vocab:\n",
    "                    vocab[token] = last_token_id\n",
    "                    last_token_id += 1\n",
    "\n",
    "    json.dump(vocab, args.vocab_json, indent=2, ensure_ascii=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9e051",
   "metadata": {},
   "source": [
    "This code adds all the special tags and turns the vocabs into json files suitable for sockeye model. Here is how we apply it for our vocabs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc68401",
   "metadata": {},
   "source": [
    "```python\n",
    "# adding tags and creating sockeye-compatible vocabs\n",
    "python subword-nmt_vocab2sockeye_vocab_adj.py --add_bt_tag --add_generic_tags --add_glossaries glossaries_file.txt vocab_file.ru > augmented_vocab_ru.json\n",
    "python subword-nmt_vocab2sockeye_vocab_adj.py --add_bt_tag --add_generic_tags --add_glossaries glossaries_file.txt vocab_file.chvkh > augmented_vocab_chvkh.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247dabce",
   "metadata": {},
   "source": [
    "As a result of all these actions we will have two files:  \n",
    "augmented_vocab_ru.json  \n",
    "augmented_vocab_chvkh.json  \n",
    "We will use these vocabs as parameters to our sockeye model.  \n",
    "In the next chapter we will talk about how we make actual BPE files for parent and child models separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e557b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
