{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3446e911",
   "metadata": {},
   "source": [
    "# WMT reproduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25674c",
   "metadata": {},
   "source": [
    "# 1. Building the vocabularies for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a40db04",
   "metadata": {},
   "source": [
    "Before creating the vocabularies I needed to prepare some files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe82dd",
   "metadata": {},
   "source": [
    "1. I needed to filter web_monolingual and witaj_monolingual Upper-Sorbian data, so that these files only have lines with those characters that are represented in other files.  \n",
    "For that I figured which are the chars that are shared in all those files by doing this (file known-hsb-cs-chars.sh, that creates known-hsb-cs-chars.txt):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be279c8",
   "metadata": {},
   "source": [
    "```python\n",
    "# known-hsb-cs-chars.sh:\n",
    "( python charset.py DGT.cs-de.cs; \\\n",
    "  python charset.py Europarl.cs-de.cs; \\\n",
    "  python charset.py News-Commentary.cs-de.cs; \\\n",
    "  python charset.py newstest2019-csde.cs; \\\n",
    "  python charset.py newstest2019-decs.cs; \\\n",
    "  python charset.py OpenSubtitles.cs-de.cs; \\\n",
    "  python charset.py WMT-News.cs-de.cs; \\\n",
    "  python charset.py sorbian_institute_monolingual.hsb; \\\n",
    "  python charset.py train.hsb-de.hsb ) | cat | sort | uniq > known-hsb-cs-chars.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0440f",
   "metadata": {},
   "source": [
    "Then I used file filtering.py as a command to filter the web_monolingual and witaj_monolingual data. Filetring.py contains this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a30c49",
   "metadata": {},
   "source": [
    "```python\n",
    "# filtering.py:\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"known-hsb-cs-chars.txt\") as f:\n",
    "        chars = set(map(str.strip, f))\n",
    "        chars.add(\" \")\n",
    "        \n",
    "    \n",
    "    \n",
    "    with open(sys.argv[1]) as f:\n",
    "        for line in map(str.strip, f):\n",
    "            if all(c in chars for c in line):\n",
    "                print(line)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea7ea1",
   "metadata": {},
   "source": [
    "2. After I filtered the files, I needed to duplicate HSB monolingual data 25 times, so that HSB data is well represented in the vocab, and the HSB tokens don't appear too rare to have a significant enough place in the vocab. Example:\n",
    "```python\n",
    "! perl -0777pe '$_=$_ x 26' train.hsb-de.de > 25train.hsb-de.de\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7e635",
   "metadata": {},
   "source": [
    "Then I concatenated all the files that I wanted to use for building the vocabularies the following way and got the combined files that I will later use in subword-nmt for building the vocabularies. At this point it doesn't matter that the file for source and target happen to be of different size.\n",
    "\n",
    "\n",
    "```python\n",
    "! cat 25train.hsb-de.de \\\n",
    "    OpenSubtitles.cs-de.de \\\n",
    "    DGT.cs-de.de \\\n",
    "    Europarl.cs-de.de \\\n",
    "    News-Commentary.cs-de.de \\\n",
    "    WMT-News.cs-de.de > train_file.de\n",
    "    \n",
    "! cat 25train.hsb-de.hsb \\\n",
    "    25sorbian_institute_monolingual.hsb \\\n",
    "    25web_monolingual_filtered.hsb \\\n",
    "    25witaj_monolingual_filtered.hsb \\\n",
    "    OpenSubtitles.cs-de.cs \\\n",
    "    DGT.cs-de.cs \\\n",
    "    Europarl.cs-de.cs \\\n",
    "    News-Commentary.cs-de.cs \\\n",
    "    WMT-News.cs-de.cs > train_file.cshsb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5bc0e",
   "metadata": {},
   "source": [
    "All of the following code is done running creating_vocabs_and_train_val_files.sh file and according to the subword-nmt tutorial.  \n",
    "First we perform learn-bpe operation on our training files. The parameters of this operation are taken from the article. After that we perform apply-bpe operation on source and target files separately to get two separate vocabs:\n",
    "\n",
    "```python\n",
    "# learn BPE on concatenation of train files and extract vocabs \n",
    "cat train_file.de train_file.cshsb | subword-nmt learn-bpe -s 20000 -o codes --min-frequency 2 --total-symbols\n",
    "\n",
    "subword-nmt apply-bpe -c codes < train_file.de | subword-nmt get-vocab > vocab_file.de\n",
    "subword-nmt apply-bpe -c codes < train_file.cshsb | subword-nmt get-vocab > vocab_file.cshsb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a9b37",
   "metadata": {},
   "source": [
    "We will explain the BPE operations in a separate notebook, here we will proceed with building the vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c2228",
   "metadata": {},
   "source": [
    "After getting the vocabs as a result of subword-nmt apply operation, we use the following script to make these vocabs compatible with sockeye model. The script is in the file subword-nmt_vocab2sockeye_vocab.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741ead2",
   "metadata": {},
   "source": [
    "```python\n",
    "# Diplom/WMT_repeat/parent_child/subword-nmt_vocab2sockeye_vocab.py\n",
    "#!/usr/bin/env  python3\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def getArgs():\n",
    "    usage=\"subword-nmt_vocac2sockeye_vocab.py  subword_nmt_vocab [sockeye_vocab_json]\"\n",
    "    help = ''\n",
    "    parser = ArgumentParser(usage=usage, description=help, add_help=True)\n",
    "    parser.add_argument(\"vocab_filename\", type=str, help=\"Subword-nmt's vocabulary filename\")\n",
    "    parser.add_argument(\"vocab_json\",\n",
    "         nargs   = '?',\n",
    "         type    = lambda f: open(f, mode='w', encoding='UTF-8'),\n",
    "         default = sys.stdout,\n",
    "         help    = \"output file [sys.stdout]\")\n",
    "    parser.add_argument(\"--add_bt_tag\",\n",
    "         dest    = \"add_bt_tag\",\n",
    "         action  = 'store_true',\n",
    "         default = False,\n",
    "         help    = \"Also include BT tag in vocab [%(default)s]\")\n",
    "    parser.add_argument(\"--add_generic_tags\",\n",
    "         dest    = \"add_generic_tags\",\n",
    "         action  = 'store_true',\n",
    "         default = False,\n",
    "         help    = \"Also include BT tag in vocab [%(default)s]\")\n",
    "    parser.add_argument(\"--add_glossaries\",\n",
    "         dest    = \"add_glossaries\",\n",
    "         nargs   = '?',\n",
    "         default = None,\n",
    "         help    = \"Also include the glossaries in vocab [%(default)s]\")\n",
    "\n",
    "\n",
    "    cmd_args = parser.parse_args()\n",
    "\n",
    "    return cmd_args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = getArgs()\n",
    "    vocab = dict({\n",
    "        \"<pad>\": 0,\n",
    "        \"<unk>\": 1,\n",
    "        \"<s>\": 2,\n",
    "        \"</s>\": 3,\n",
    "        })\n",
    "\n",
    "    if args.add_bt_tag:\n",
    "        vocab.update({'<BT>': len(vocab)})\n",
    "\n",
    "    if args.add_generic_tags:\n",
    "        vocab.update({ f'<TAG{i:02}>' : i+len(vocab)-1 for i in range(1, 26) })\n",
    "\n",
    "    with open(args.vocab_filename, mode='r', encoding='UTF-8') as f:\n",
    "        vocab.update({ k: v for v, k in enumerate(map(lambda l: l.strip().split()[0], f.readlines()), start=len(vocab.keys())) })\n",
    "\n",
    "    if args.add_glossaries is not None:\n",
    "        last_token_id = len(vocab.keys())\n",
    "        with open(args.add_glossaries, mode='rt') as f:\n",
    "            for token in map(str.strip, f.readlines()):\n",
    "                if token != '' and token not in vocab:\n",
    "                    vocab[token] = last_token_id\n",
    "                    last_token_id += 1\n",
    "\n",
    "    json.dump(vocab, args.vocab_json, indent=2, ensure_ascii=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9e051",
   "metadata": {},
   "source": [
    "This code adds all the special tags and turns the vocabs into json files suitable for sockeye model. Here is how we apply it for our vocabs:\n",
    "```python\n",
    "# adding tags and creating sockeye-compatible vocabs\n",
    "python subword_nmt_vocab2sockeye_vocab.py --add_bt_tag --add_generic_tags --add_glossaries glossaries_file.txt vocab_file.de > augmented_vocab_de.json\n",
    "python subword_nmt_vocab2sockeye_vocab.py --add_bt_tag --add_generic_tags --add_glossaries glossaries_file.txt vocab_file.cshsb > augmented_vocab_cshsb.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247dabce",
   "metadata": {},
   "source": [
    "We will use these vocabs as parameters to our sockeye model.  \n",
    "In the next chapter we will talk about how we make actual BPE files for parent and child models separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
